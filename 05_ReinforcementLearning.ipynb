{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"images/vbti_logo.png\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Reinforcement Learning\n",
    "This notebook supports the 'Reinforcement Learning' chapter of the [1-day masterclass \"Deep Learning\"](https://aiblog.nl/masterclass-deep-learning). It is not ment as a full course on deep learning, but rather gives you a flavor of the topic. For an in-depth AI training or consultancy please contact [VBTI](https://vbti.nl). \n",
    "\n",
    "Reinforcement Learning is an AI technique to learn an optimal sequence of actions. During the masterclass details of the action-value method, Q-learning, exploration/exploitation, discount factor and Deep Reinforcement Learning are explained. In this notebook you will build and train a RL agent that needs to optimize planning taxi trips.\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://aiblog.nl/masterclass-deep-learning\"><img src=\"images/rl.png\" width=\"400\"></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some default libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment preparation\n",
    "In this example the [gym tool](https://gym.openai.com) is used to create a simulation environment in which an agent needs to learn a task. The environment used is called 'Taxi-v3'. In this toy environment the agents needs to drive a taxi to pick up and drop passengers. \n",
    "\n",
    "> There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions. (['Taxi-v3'](https://gym.openai.com/envs/Taxi-v3/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions : Discrete(6)\n",
      "Number of states  : Discrete(500)\n",
      "Initial state     : 327\n"
     ]
    }
   ],
   "source": [
    "# import and create the simulation environment\n",
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "print(\"Number of actions : {}\".format(env.action_space))\n",
    "print(\"Number of states  : {}\".format(env.observation_space))\n",
    "\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state     : {}\".format(initial_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# render environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Agent:    \n",
    "    \"\"\"Abstract Agent class.\"\"\"\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        \n",
    "    def initialize_episode(self):\n",
    "        self.sum_rewards = 0\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return None\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        self.sum_rewards = self.sum_rewards + reward\n",
    "    \n",
    "    def finalize_episode(self):\n",
    "        self.rewards.append(self.sum_rewards)\n",
    "        \n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def plot_rewards(self, ylim=(-100,0)):\n",
    "        plt.figure(figsize=(18,5))\n",
    "        plt.plot(self.rewards)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('sum of rewards')\n",
    "        plt.ylim(ylim)\n",
    "        plt.title(self.get_name())\n",
    "        \n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_experiment(environment, agent, n_experiments=1, max_steps=100, render=False, sleep=0.01, n_epoch_update=1000, plot_stats=False):\n",
    "    # do some bookkeeping\n",
    "    stats_steps = []\n",
    "    stats_rewards = []\n",
    "    stats_penalties = []\n",
    "    stats_reward_per_step = []\n",
    "    \n",
    "    for n in range(n_experiments):\n",
    "        # reset environment and agent for this episode\n",
    "        state = environment.reset()\n",
    "        agent.initialize_episode()\n",
    "        \n",
    "        # render environment\n",
    "        if render:\n",
    "            environment.render()\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(sleep)\n",
    "        elif n % n_epoch_update ==0: # print progress\n",
    "            print(\"Run experiment : {} / {}\".format(n, n_experiments))\n",
    "        \n",
    "        # episode loop\n",
    "        steps = 0\n",
    "        penalties = 0\n",
    "        done = False\n",
    "        while (not done) and (steps < max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            agent.update(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            steps = steps + 1\n",
    "            \n",
    "            if render:\n",
    "                environment.render()\n",
    "                clear_output(wait=True)\n",
    "                time.sleep(sleep)\n",
    "             \n",
    "            # count penalties\n",
    "            if reward==-10:\n",
    "                penalties += 1\n",
    "            \n",
    "        agent.finalize_episode()\n",
    "        stats_penalties.append(penalties)\n",
    "        stats_steps.append(steps)\n",
    "        stats_rewards.append(agent.sum_rewards)\n",
    "        stats_reward_per_step.append(agent.sum_rewards / steps)\n",
    "    \n",
    "    if render:\n",
    "        environment.render()\n",
    "        print('')\n",
    "    else:\n",
    "        print('Done.\\n')\n",
    "\n",
    "    print(\"Average reward       : {}\".format(np.mean(stats_rewards)))\n",
    "    print(\"Average #penalties   : {}\".format(np.mean(stats_penalties)))\n",
    "    print(\"Average #steps       : {}\".format(np.mean(stats_steps)))\n",
    "    print(\"Average #reward/step : {}\".format(np.mean(stats_reward_per_step)))\n",
    "    \n",
    "    if plot_stats:\n",
    "        plt.plot(stats_steps, label='#steps')\n",
    "        plt.plot(stats_penalties, label='#penalties')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Q-learning Agent\n",
    "In the masterclass the Q-learning technique is explained. Following this methods, an agent learns to evaluate a state-action combination by using the following rule:\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ R_{t+1} +\\gamma \\max\\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\n",
    "$$\n",
    "\n",
    "The code below implements this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, n_states, n_actions, epsilon=0.1, gamma=0.9, alpha=0.1):\n",
    "        Agent.__init__(self)\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.learn = True\n",
    "                \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        Agent.reset(self)                \n",
    "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "                \n",
    "    def get_action(self, state):\n",
    "        # e-greedy\n",
    "        if self.learn & (np.random.random()<self.epsilon): # explore\n",
    "            a = np.random.randint(self.n_actions)\n",
    "        else: # exploit\n",
    "            a = np.argmax(self.Q[state,:])\n",
    "        return a\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        if self.learn:\n",
    "            self.Q[state, action] = self.Q[state, action] + self.alpha * (reward + \n",
    "                                                                          self.gamma * np.max(self.Q[next_state,:]) -\n",
    "                                                                          self.Q[state, action])\n",
    "        Agent.update(self, state, action, next_state, reward)"
   ]
  },
  {
