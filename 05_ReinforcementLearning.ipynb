{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"images/vbti_logo.png\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Reinforcement Learning\n",
    "This notebook supports the 'Reinforcement Learning' chapter of the [1-day masterclass \"Deep Learning\"](https://aiblog.nl/masterclass-deep-learning). It is not ment as a full course on deep learning, but rather gives you a flavor of the topic. For an in-depth AI training or consultancy please contact [VBTI](https://vbti.nl). \n",
    "\n",
    "Reinforcement Learning is an AI technique to learn an optimal sequence of actions. During the masterclass details of the action-value method, Q-learning, exploration/exploitation, discount factor and Deep Reinforcement Learning are explained. In this notebook you will build and train a RL agent that needs to optimize planning taxi trips.\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://aiblog.nl/masterclass-deep-learning\"><img src=\"images/rl.png\" width=\"400\"></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some default libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment preparation\n",
    "In this example the [gym tool](https://gym.openai.com) is used to create a simulation environment in which an agent needs to learn a task. The environment used is called 'Taxi-v3'. In this toy environment the agents needs to drive a taxi to pick up and drop passengers. \n",
    "\n",
    "> There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions. (['Taxi-v3'](https://gym.openai.com/envs/Taxi-v3/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions : Discrete(6)\n",
      "Number of states  : Discrete(500)\n",
      "Initial state     : 327\n"
     ]
    }
   ],
   "source": [
    "# import and create the simulation environment\n",
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "print(\"Number of actions : {}\".format(env.action_space))\n",
    "print(\"Number of states  : {}\".format(env.observation_space))\n",
    "\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state     : {}\".format(initial_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# render environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Agent:    \n",
    "    \"\"\"Abstract Agent class.\"\"\"\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        \n",
    "    def initialize_episode(self):\n",
    "        self.sum_rewards = 0\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return None\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        self.sum_rewards = self.sum_rewards + reward\n",
    "    \n",
    "    def finalize_episode(self):\n",
    "        self.rewards.append(self.sum_rewards)\n",
    "        \n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def plot_rewards(self, ylim=(-100,0)):\n",
    "        plt.figure(figsize=(18,5))\n",
    "        plt.plot(self.rewards)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('sum of rewards')\n",
    "        plt.ylim(ylim)\n",
    "        plt.title(self.get_name())\n",
    "        \n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_experiment(environment, agent, n_experiments=1, max_steps=100, render=False, sleep=0.01, n_epoch_update=1000, plot_stats=False):\n",
    "    # do some bookkeeping\n",
    "    stats_steps = []\n",
    "    stats_rewards = []\n",
    "    stats_penalties = []\n",
    "    stats_reward_per_step = []\n",
    "    \n",
    "    for n in range(n_experiments):\n",
    "        # reset environment and agent for this episode\n",
    "        state = environment.reset()\n",
    "        agent.initialize_episode()\n",
    "        \n",
    "        # render environment\n",
    "        if render:\n",
    "            environment.render()\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(sleep)\n",
    "        elif n % n_epoch_update ==0: # print progress\n",
    "            print(\"Run experiment : {} / {}\".format(n, n_experiments))\n",
    "        \n",
    "        # episode loop\n",
    "        steps = 0\n",
    "        penalties = 0\n",
    "        done = False\n",
    "        while (not done) and (steps < max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            agent.update(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            steps = steps + 1\n",
    "            \n",
    "            if render:\n",
    "                environment.render()\n",
    "                clear_output(wait=True)\n",
    "                time.sleep(sleep)\n",
    "             \n",
    "            # count penalties\n",
    "            if reward==-10:\n",
    "                penalties += 1\n",
    "            \n",
    "        agent.finalize_episode()\n",
    "        stats_penalties.append(penalties)\n",
    "        stats_steps.append(steps)\n",
    "        stats_rewards.append(agent.sum_rewards)\n",
    "        stats_reward_per_step.append(agent.sum_rewards / steps)\n",
    "    \n",
    "    if render:\n",
    "        environment.render()\n",
    "        print('')\n",
    "    else:\n",
    "        print('Done.\\n')\n",
    "\n",
    "    print(\"Average reward       : {}\".format(np.mean(stats_rewards)))\n",
    "    print(\"Average #penalties   : {}\".format(np.mean(stats_penalties)))\n",
    "    print(\"Average #steps       : {}\".format(np.mean(stats_steps)))\n",
    "    print(\"Average #reward/step : {}\".format(np.mean(stats_reward_per_step)))\n",
    "    \n",
    "    if plot_stats:\n",
    "        plt.plot(stats_steps, label='#steps')\n",
    "        plt.plot(stats_penalties, label='#penalties')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Q-learning Agent\n",
    "In the masterclass the Q-learning technique is explained. Following this methods, an agent learns to evaluate a state-action combination by using the following rule:\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ R_{t+1} +\\gamma \\max\\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\n",
    "$$\n",
    "\n",
    "The code below implements this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, n_states, n_actions, epsilon=0.1, gamma=0.9, alpha=0.1):\n",
    "        Agent.__init__(self)\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.learn = True\n",
    "                \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        Agent.reset(self)                \n",
    "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "                \n",
    "    def get_action(self, state):\n",
    "        # e-greedy\n",
    "        if self.learn & (np.random.random()<self.epsilon): # explore\n",
    "            a = np.random.randint(self.n_actions)\n",
    "        else: # exploit\n",
    "            a = np.argmax(self.Q[state,:])\n",
    "        return a\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        if self.learn:\n",
    "            self.Q[state, action] = self.Q[state, action] + self.alpha * (reward + \n",
    "                                                                          self.gamma * np.max(self.Q[next_state,:]) -\n",
    "                                                                          self.Q[state, action])\n",
    "        Agent.update(self, state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Q-learning agent\n",
    "agent_Q = QLearningAgent(n_states=500, n_actions=6, epsilon=0.1, alpha=0.1, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent\n",
    "To train a Q-learning agent it needs to try to drive the taxi many times. A maximum duration of 200 steps will be allowed to pick up and drop a passenger. \n",
    "\n",
    "First, let's see how well the agents performance without learning. The average performance over 10 experimens is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "Average reward       : -200.0\n",
      "Average #penalties   : 0.0\n",
      "Average #steps       : 200.0\n",
      "Average #reward/step : -1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate performance of agent\n",
    "# Note: sometimes the agent does not move but the simulation does run. Be patient! ;)\n",
    "agent_Q.learn = False\n",
    "run_experiment(env, agent_Q, n_experiments=10, max_steps=200, render=True)\n",
    "agent_Q.learn = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely, the agent will performance very bad (average reward: -200, average number of steps: 200). Therefore 20.000 experiments are carried out during which the agent learns to carry out its job at best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run experiment : 0 / 20000\n",
      "Run experiment : 10000 / 20000\n",
      "Done.\n",
      "\n",
      "Average reward       : -2.03835\n",
      "Average #penalties   : 0.6131\n",
      "Average #steps       : 17.3745\n",
      "Average #reward/step : 0.27727722386973236\n"
     ]
    },
    {
     "data": {
      "image/png": "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