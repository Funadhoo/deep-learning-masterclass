{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"images/vbti_logo.png\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Reinforcement Learning\n",
    "This notebook supports the 'Reinforcement Learning' chapter of the [1-day masterclass \"Deep Learning\"](https://aiblog.nl/masterclass-deep-learning). It is not ment as a full course on deep learning, but rather gives you a flavor of the topic. For an in-depth AI training or consultancy please contact [VBTI](https://vbti.nl). \n",
    "\n",
    "Reinforcement Learning is an AI technique to learn an optimal sequence of actions. During the masterclass details of the action-value method, Q-learning, exploration/exploitation, discount factor and Deep Reinforcement Learning are explained. In this notebook you will build and train a RL agent that needs to optimize planning taxi trips.\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://aiblog.nl/masterclass-deep-learning\"><img src=\"images/rl.png\" width=\"400\"></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some default libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment preparation\n",
    "In this example the [gym tool](https://gym.openai.com) is used to create a simulation environment in which an agent needs to learn a task. The environment used is called 'Taxi-v3'. In this toy environment the agents needs to drive a taxi to pick up and drop passengers. \n",
    "\n",
    "> There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions. (['Taxi-v3'](https://gym.openai.com/envs/Taxi-v3/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions : Discrete(6)\n",
      "Number of states  : Discrete(500)\n",
      "Initial state     : 327\n"
     ]
    }
   ],
   "source": [
    "# import and create the simulation environment\n",
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "print(\"Number of actions : {}\".format(env.action_space))\n",
    "print(\"Number of states  : {}\".format(env.observation_space))\n",
    "\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state     : {}\".format(initial_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# render environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Agent:    \n",
    "    \"\"\"Abstract Agent class.\"\"\"\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        \n",
    "    def initialize_episode(self):\n",
    "        self.sum_rewards = 0\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return None\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        self.sum_rewards = self.sum_rewards + reward\n",
    "    \n",
    "    def finalize_episode(self):\n",
    "        self.rewards.append(self.sum_rewards)\n",
    "        \n",
    "    def get_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def plot_rewards(self, ylim=(-100,0)):\n",
    "        plt.figure(figsize=(18,5))\n",
    "        plt.plot(self.rewards)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('sum of rewards')\n",
    "        plt.ylim(ylim)\n",
    "        plt.title(self.get_name())\n",
    "        \n",
    "\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_experiment(environment, agent, n_experiments=1, max_steps=100, render=False, sleep=0.01, n_epoch_update=1000, plot_stats=False):\n",
    "    # do some bookkeeping\n",
    "    stats_steps = []\n",
    "    stats_rewards = []\n",
    "    stats_penalties = []\n",
    "    stats_reward_per_step = []\n",
    "    \n",
    "    for n in range(n_experiments):\n",
    "        # reset environment and agent for this episode\n",
    "        state = environment.reset()\n",
    "        agent.initialize_episode()\n",
    "        \n",
    "        # render environment\n",
    "        if render:\n",
    "            environment.render()\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(sleep)\n",
    "        elif n % n_epoch_update ==0: # print progress\n",
    "            print(\"Run experiment : {} / {}\".format(n, n_experiments))\n",
    "        \n",
    "        # episode loop\n",
    "        steps = 0\n",
    "        penalties = 0\n",
    "        done = False\n",
    "        while (not done) and (steps < max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            agent.update(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            steps = steps + 1\n",
    "            \n",
    "            if render:\n",
    "                environment.render()\n",
    "                clear_output(wait=True)\n",
    "                time.sleep(sleep)\n",
    "             \n",
    "            # count penalties\n",
    "            if reward==-10:\n",
    "                penalties += 1\n",
    "            \n",
    "        agent.finalize_episode()\n",
    "        stats_penalties.append(penalties)\n",
    "        stats_steps.append(steps)\n",
    "        stats_rewards.append(agent.sum_rewards)\n",
    "        stats_reward_per_step.append(agent.sum_rewards / steps)\n",
    "    \n",
    "    if render:\n",
    "        environment.render()\n",
    "        print('')\n",
    "    else:\n",
    "        print('Done.\\n')\n",
    "\n",
    "    print(\"Average reward       : {}\".format(np.mean(stats_rewards)))\n",
    "    print(\"Average #penalties   : {}\".format(np.mean(stats_penalties)))\n",
    "    print(\"Average #steps       : {}\".format(np.mean(stats_steps)))\n",
    "    print(\"Average #reward/step : {}\".format(np.mean(stats_reward_per_step)))\n",
    "    \n",
    "    if plot_stats:\n",
    "        plt.plot(stats_steps, label='#steps')\n",
    "        plt.plot(stats_penalties, label='#penalties')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Q-learning Agent\n",
    "In the masterclass the Q-learning technique is explained. Following this methods, an agent learns to evaluate a state-action combination by using the following rule:\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ R_{t+1} +\\gamma \\max\\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\n",
    "$$\n",
    "\n",
    "The code below implements this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, n_states, n_actions, epsilon=0.1, gamma=0.9, alpha=0.1):\n",
    "        Agent.__init__(self)\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.learn = True\n",
    "                \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        Agent.reset(self)                \n",
    "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "                \n",
    "    def get_action(self, state):\n",
    "        # e-greedy\n",
    "        if self.learn & (np.random.random()<self.epsilon): # explore\n",
    "            a = np.random.randint(self.n_actions)\n",
    "        else: # exploit\n",
    "            a = np.argmax(self.Q[state,:])\n",
    "        return a\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        if self.learn:\n",
    "            self.Q[state, action] = self.Q[state, action] + self.alpha * (reward + \n",
    "                                                                          self.gamma * np.max(self.Q[next_state,:]) -\n",
    "                                                                          self.Q[state, action])\n",
    "        Agent.update(self, state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Q-learning agent\n",
    "agent_Q = QLearningAgent(n_states=500, n_actions=6, epsilon=0.1, alpha=0.1, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent\n",
    "To train a Q-learning agent it needs to try to drive the taxi many times. A maximum duration of 200 steps will be allowed to pick up and drop a passenger. \n",
    "\n",
    "First, let's see how well the agents performance without learning. The average performance over 10 experimens is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "Average reward       : -200.0\n",
      "Average #penalties   : 0.0\n",
      "Average #steps       : 200.0\n",
      "Average #reward/step : -1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate performance of agent\n",
    "# Note: sometimes the agent does not move but the simulation does run. Be patient! ;)\n",
    "agent_Q.learn = False\n",
    "run_experiment(env, agent_Q, n_experiments=10, max_steps=200, render=True)\n",
    "agent_Q.learn = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely, the agent will performance very bad (average reward: -200, average number of steps: 200). Therefore 20.000 experiments are carried out during which the agent learns to carry out its job at best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run experiment : 0 / 20000\n",
      "Run experiment : 10000 / 20000\n",
      "Done.\n",
      "\n",
      "Average reward       : -2.03835\n",
      "Average #penalties   : 0.6131\n",
      "Average #steps       : 17.3745\n",
      "Average #reward/step : 0.27727722386973236\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4FeX1wPHvYRFQEBAiioggRRQQAiKKC7JYRS1CbasiKtYF959Lq0IRobRarEutYsUFVBQVFBAFlE12CRBCWMIWIAECAUJYAoFAlvP7YyaXG7LfJTe5OZ/nyZO578y8c2bu3HNn3pn7jqgqxhhjwleVUAdgjDEmuCzRG2NMmLNEb4wxYc4SvTHGhDlL9MYYE+Ys0RtjTJizRG+MMWHOEr0xxoQ5S/TGGBPmqoU6AICGDRtqs2bNQh2GMcZUKCtXrtyvqhHFTVcuEn2zZs2Ijo4OdRjGGFOhiMj2kkxnTTfGGBPmLNEbY0yYs0RvjDFhrly00RtjKq7MzEySkpLIyMgIdShhq2bNmjRp0oTq1av7NL8lemOMX5KSkqhTpw7NmjVDREIdTthRVVJTU0lKSqJ58+Y+1VFs042IXCgi80RkvYjEicgzbvk5IjJbROLd//XdchGRd0Vki4isEZGOPkVmjKkQMjIyaNCggSX5IBERGjRo4NcZU0na6LOAv6hqa+Bq4EkRaQ0MAuaqaktgrvsa4Bagpfs3EPjA5+iMMRWCJfng8nf7Ftt0o6rJQLI7fERENgAXAH2Abu5knwPzgZfc8nHqPKMwSkTqicj5bj0BtedwBlf